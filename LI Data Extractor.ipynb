{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "pp = pprint.PrettyPrinter(indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO:\n",
    "    \n",
    "\n",
    "## DONE:\n",
    "    #Crawl local folder for html files\n",
    "    #Solve BeautifulSoup missing content error\n",
    "    #define functions\n",
    "    #JSON vs csv multi var assignment\n",
    "    #build dataframe (or JSON)\n",
    "    #integrate into dataframe (adding columns for previously not there for multi var assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sample files\n",
    "\n",
    "#soup = BeautifulSoup(open(\"/Users/Matzekoek/Dropbox/Datascience/6. Projects/4. LinkedIn Data Extractor/profiles/LI Profiel - Michaël Jeckmans.htm\"), \"html.parser\")\n",
    "#soup = BeautifulSoup(open(\"/Users/Matzekoek/Dropbox/Datascience/6. Projects/4. LinkedIn Data Extractor/LI Profiel - Michaël Jeckmans.htm\"), \"html.parser\")\n",
    "#soup = BeautifulSoup(open(\"C:\\\\Users\\\\Matthias\\\\Dropbox\\\\Datascience\\\\6. Projects\\\\4. LinkedIn Data Extractor\\\\LI Profiel - Michaël Jeckmans.htm\"), \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBaseInfo():\n",
    "    \n",
    "    if soup.find(\"h1\", class_=\"searchable\") is None:\n",
    "        name = ''\n",
    "    else:\n",
    "        name = soup.find(\"h1\", class_=\"searchable\").text\n",
    "\n",
    "    if soup.find(\"span\", class_=\"location searchable\") is None:\n",
    "        town = ''\n",
    "    else:\n",
    "        town = soup.find(\"span\", class_=\"location searchable\").text\n",
    "        \n",
    "    if soup.find(\"span\", class_=\"industry searchable\") is None:\n",
    "        industry = ''\n",
    "    else:\n",
    "        industry = soup.find(\"span\", class_=\"industry searchable\").text\n",
    "\n",
    "    if soup.find(\"div\", class_=\"module-body searchable\") is None:\n",
    "        summary = ''\n",
    "    else:\n",
    "        summary = soup.find(\"div\", class_=\"module-body searchable\").text\n",
    "    return name, town, industry, summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job experience\n",
    "\n",
    "def getExperience():\n",
    "\n",
    "    jobResults = soup.select('div[id=\"profile-experience\"] > div > ul > li')\n",
    "    jobs = []\n",
    "\n",
    "    for i in jobResults: \n",
    "        subSoup = BeautifulSoup(str(i), \"html.parser\")\n",
    "    \n",
    "    #check for Nonetype, if true: variable = ''\n",
    "    \n",
    "        if subSoup.find(\"h4\", class_=\"searchable\") is None:\n",
    "            jobTitle = ''\n",
    "        else: \n",
    "            jobTitle = subSoup.find(\"h4\", class_=\"searchable\").text\n",
    "    \n",
    "        if subSoup.find(\"h5\", class_=\"searchable\") is None:\n",
    "            jobCompany = ''\n",
    "        else: \n",
    "            jobCompany = subSoup.find(\"h5\", class_=\"searchable\").text\n",
    "    \n",
    "        if subSoup.find(\"p\", class_=\"date-range\") is None:\n",
    "            jobDateRange = ''\n",
    "        else: \n",
    "            jobDateRange = subSoup.find(\"p\", class_=\"date-range\").text\n",
    "    \n",
    "        if subSoup.find(\"span\", class_=\"duration\") is None:\n",
    "            jobDuration = ''\n",
    "        else: \n",
    "            jobDuration = subSoup.find(\"span\", class_=\"duration\").text\n",
    "    \n",
    "        if subSoup.find(\"span\", class_=\"location\") is None:\n",
    "            jobLocation = ''\n",
    "        else: \n",
    "            jobLocation = subSoup.find(\"span\", class_=\"location\").text\n",
    "    \n",
    "        if subSoup.find(\"p\", class_=\"description\") is None:\n",
    "            jobDescription = ''\n",
    "        else: \n",
    "            jobDescription = subSoup.find(\"p\", class_=\"description\").text\n",
    "\n",
    "        job = {\n",
    "            \"Job Title\" : jobTitle,\n",
    "            \"Company\" : jobCompany,\n",
    "            \"Job Date Range\" : jobDateRange,\n",
    "            \"Job Duration\" : jobDuration,\n",
    "            \"Job Location\" : jobLocation,\n",
    "            \"Job Description\" : jobDescription\n",
    "        }\n",
    "        jobs.append(job)\n",
    "    return jobs\n",
    "\n",
    "#getExperience()\n",
    "#pp.pprint(jobs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEducation():    \n",
    "    \n",
    "    educationResults = soup.select('div[id=\"profile-education\"] > div > ul > li') \n",
    "    educations = []\n",
    "    \n",
    "    for i in educationResults: \n",
    "        subSoup = BeautifulSoup(str(i), \"html.parser\")\n",
    "        \n",
    "        #check for Nonetype, if true: variable = ''\n",
    "        if subSoup.find(\"h4\", class_=\"searchable\") is None:\n",
    "            institution = ''\n",
    "        else: \n",
    "            institution = subSoup.find(\"h4\", class_=\"searchable\").text\n",
    "\n",
    "        if subSoup.find(\"h5\", class_=\"searchable\") is None:\n",
    "            study = ''\n",
    "        else: \n",
    "            study = subSoup.find(\"h5\", class_=\"searchable\").text\n",
    "\n",
    "        if subSoup.find(\"p\", class_=\"date-range\") is None:\n",
    "            studyDateRange = ''\n",
    "        else: \n",
    "            studyDateRange = subSoup.find(\"p\", class_=\"date-range\").text\n",
    "\n",
    "        if subSoup.find(\"p\", class_=\"description\") is None:\n",
    "            studyDescription = ''\n",
    "        else: \n",
    "            studyDescription = subSoup.find(\"p\", class_=\"description\").text\n",
    "\n",
    "        education = {\n",
    "            \"institution\" : institution,\n",
    "            \"study\" : study,\n",
    "            \"studyDateRange\" : studyDateRange,\n",
    "            \"studyDescription\" : studyDescription\n",
    "        }\n",
    "        educations.append(education)\n",
    "    return educations\n",
    "       \n",
    "#getEducation()\n",
    "#pp.pprint(educations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#languages datagrab\n",
    "\n",
    "def getLanguages():\n",
    "\n",
    "    languageResults = soup.select('div[id=\"profile-language\"] > div > ul > li') \n",
    "    \n",
    "    languages = []\n",
    "    for i in languageResults: \n",
    "        subSoup = BeautifulSoup(str(i), \"html.parser\")\n",
    "\n",
    "        #if type(subSoup.find(\"h4\", class_=\"searchable\")) == None:\n",
    "        #    language = ''\n",
    "        #else: \n",
    "        language = subSoup.find(\"h4\", class_=\"searchable\").text\n",
    "\n",
    "        try:\n",
    "            if type(subSoup.find(\"p\", class_=\"proficiency\")) is None:\n",
    "                proficiency = ''\n",
    "            else: \n",
    "                proficiency = subSoup.find(\"p\", class_=\"proficiency\").text\n",
    "        except AttributeError:\n",
    "            proficiency = ''\n",
    "\n",
    "        language = {\n",
    "            \"Language\" : language,\n",
    "            \"Proficiency\" : proficiency\n",
    "        }\n",
    "        languages.append(language)\n",
    "    return languages\n",
    "\n",
    "#getLanguages()\n",
    "#pp.pprint(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skills datagrab\n",
    "\n",
    "def getSkills():\n",
    "\n",
    "    skillsResults = soup.select('div[id=\"profile-skills\"] > div > ul > li') \n",
    "\n",
    "    skills = []\n",
    "    for i in skillsResults:\n",
    "        skills.append(i.text)\n",
    "    \n",
    "    return skills\n",
    "\n",
    "#getSkills()\n",
    "#pp.pprint(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groups datagrab\n",
    "\n",
    "def getGroups():\n",
    "\n",
    "    groupsResults = soup.select('div[id=\"profile-groups\"] > div > ul > li > p[class=\"group-name\"]') \n",
    "\n",
    "    groups = []\n",
    "    for i in groupsResults:\n",
    "        groups.append(i.text)\n",
    "    return groups\n",
    "\n",
    "#getGroups()\n",
    "#pp.pprint(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/Matzekoek/Dropbox/Datascience/6. Projects/4. LinkedIn Data Extractor/profielen monteur e apeldoorn ht\"\n",
    "files = []\n",
    "for filename in os.listdir(path):\n",
    "    if str(filename).endswith(\"htm\"):\n",
    "        files.append(str(filename))\n",
    "\n",
    "data = {}\n",
    "i = 1\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    soup = BeautifulSoup(open(str(os.path.join(path,file))))\n",
    "    \n",
    "    name, town, industry, summary = getBaseInfo()\n",
    "    jobs = getExperience()\n",
    "    educations = getEducation()\n",
    "    languages = getLanguages()\n",
    "    skills = getSkills()\n",
    "    groups = getGroups()\n",
    "    \n",
    "    data[i] = {str(i) : { \n",
    "    \"Name\": name,\n",
    "    \"Location\" : town,\n",
    "    \"Industry\" : industry,\n",
    "    \"Profile Summary\" : summary,\n",
    "    \"Experience\": jobs,\n",
    "    \"Education\" : educations,\n",
    "    \"Languages\" : languages,\n",
    "    \"Skills\" : skills,\n",
    "    \"Groups\" : groups\n",
    "       }}\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_file_linkedin.json\", \"w\") as write_file:\n",
    "    json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#junk/not working code\n",
    "\n",
    "'''\n",
    "\n",
    "#print(name)\n",
    "#print(town)\n",
    "#print(industry)\n",
    "#print(summary)\n",
    "        \n",
    "#name = soup.find(\"h1\", class_=\"searchable\").text\n",
    "#town = soup.find(\"span\", class_=\"location searchable\").text\n",
    "#industry = soup.find(\"span\", class_=\"industry searchable\").text\n",
    "#summary = soup.find(\"div\", class_=\"module-body searchable\").text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(open(\"/Users/Matzekoek/Dropbox/Datascience/6. Projects/4. LinkedIn Data Extractor/profielen monteur e apeldoorn ht/Andre Gimbergh.htm\"), \"html.parser\")\n",
    "\n",
    "def getLanguages():\n",
    "\n",
    "    languageResults = soup.select('div[id=\"profile-language\"] > div > ul > li') \n",
    "    \n",
    "    global languages\n",
    "    languages = []\n",
    "    for i in languageResults: \n",
    "        subSoup = BeautifulSoup(str(i), \"html.parser\")\n",
    "\n",
    "        #if type(subSoup.find(\"h4\", class_=\"searchable\")) == None:\n",
    "        #    language = ''\n",
    "        #else: \n",
    "        \n",
    "        if type(subSoup.find(\"h4\", class_=\"searchable\")) == None:\n",
    "            language = ''\n",
    "        else: \n",
    "            language = subSoup.find(\"h4\", class_=\"searchable\").text\n",
    "\n",
    "        if type(subSoup.find(\"p\", class_=\"proficiency\")) == None:\n",
    "            proficiency = ''\n",
    "        else: \n",
    "            proficiency = subSoup.find(\"p\", class_=\"proficiency\").text\n",
    "\n",
    "        language = {\n",
    "            \"Language\" : language,\n",
    "            \"Proficiency\" : proficiency\n",
    "        }\n",
    "        languages.append(language)\n",
    "    return languages\n",
    "    \n",
    " for ervaringen in data[1]['Experience']:\n",
    "    print(ervaringen) \n",
    "    \n",
    "for k,v in data[3]['3']['Education']:\n",
    "    print(k)\n",
    "\n",
    "getBaseInfo() \n",
    "getExperience()\n",
    "getEducation()\n",
    "getLanguages()\n",
    "getSkills()\n",
    "getGroups()\n",
    "    \n",
    "\n",
    "#studyDateRange = subSoup.find(\"p\", class_=\"date-range\").text\n",
    "        #studyDescription = subSoup.find(\"p\", class_=\"description\").text\n",
    "\n",
    "\n",
    "# #for folderName, subfolders, filenames in os.walk(\"/Users/Matzekoek/Dropbox/Datascience/6. Projects/4. LinkedIn Data Extractor\"):\n",
    "# #    for filename in filenames:\n",
    "# #        if filename.endswith(\"htm\"):\n",
    "# #            print(filename)\n",
    "            \n",
    "# path = \"/Users/Matzekoek/Dropbox/Datascience/6. Projects/4. LinkedIn Data Extractor/profiles\"\n",
    "# files = []\n",
    "# for filename in os.listdir(path):\n",
    "#     if str(filename).endswith(\"htm\"):\n",
    "#         files.append(str(filename))\n",
    "    \n",
    "# i = 0\n",
    "# for file in files:\n",
    "#     soup = BeautifulSoup(open(str(os.path.join(path,file))))\n",
    "\n",
    "pp.pprint(name)\n",
    "pp.pprint(town)\n",
    "pp.pprint(industry)\n",
    "pp.pprint(summary)\n",
    "pp.pprint(jobs)\n",
    "pp.pprint(educations)\n",
    "pp.pprint(languages)\n",
    "pp.pprint(skills)\n",
    "pp.pprint(groups)\n",
    "\n",
    "jobtitleresults = soup.select('div[id=\"profile-experience\"] > div > ul > li > div > h4[class=\"searchable\"]')\n",
    "\n",
    "soup.select(\"div[id=foo] > div > div > div[class=fee] > span > span > a\")\n",
    "\n",
    "\n",
    "company\n",
    "joblengthdate\n",
    "joblength\n",
    "jobtown\n",
    "jobdescription\n",
    "\n",
    "soup.select('input[type=\"button\"]')\n",
    "\n",
    "\n",
    "df['Woonplaats'] = np.where(df['Naam'] == name, town, df['Woonplaats'])\n",
    "\n",
    " #study = subSoup.find(\"h5\", class_=\"searchable\")\n",
    "        #studydaterange = subSoup.find(\"p\", class_=\"date-range\")\n",
    "        #studydescription = subSoup.find(\"p\", class_=\"description\")\n",
    "\n",
    "\n",
    "eduInstituteResults = soup.select('div[id=\"profile-education\"] > div > ul > li > div > h4[class=\"searchable\"]')\n",
    "eduInstitute = []\n",
    "for i in eduInstituteResults:\n",
    "    eduInstitute.append(i.text)\n",
    "\n",
    "\n",
    "print(len(eduInstitute))\n",
    "print(len(test))\n",
    "\n",
    "#Get the data\n",
    "\n",
    "name = soup.find(\"h1\", class_=\"searchable\")\n",
    "nametxt = name.text\n",
    "\n",
    "town = soup.find(\"span\", class_=\"location searchable\")\n",
    "towntxt = town.text\n",
    "\n",
    "industry = soup.find(\"span\", class_=\"industry searchable\")\n",
    "industrytxt = industry.text\n",
    "\n",
    "summary = soup.find(\"div\", class_=\"module-body searchable\")\n",
    "summarytxt = summary.text\n",
    "\n",
    "jobtitleresults = soup.select('div[id=\"profile-experience\"] > div > ul > li > div > h4[class=\"searchable\"]')\n",
    "\n",
    "jobtitles = []\n",
    "for i in jobtitleresults:\n",
    "    jobtitles.append(i.text)\n",
    "\n",
    "    \n",
    "employersresults = soup.select('div[id=\"profile-experience\"] > div > ul > li > div > h5[class=\"searchable\"]')\n",
    "employers = []\n",
    "for i in employersresults:\n",
    "    employers.append(i.text)\n",
    "\n",
    "\n",
    "edusummaryresults = soup.select('div[id=\"profile-education\"] > div > ul > li > p[class*=\"description\"]')    \n",
    "test = []\n",
    "for i in edusummarytest:\n",
    "    test.append(i.text)\n",
    "\n",
    "\n",
    "#study = subSoup.find(\"h5\", class_=\"searchable\")\n",
    "#studydaterange = subSoup.find(\"p\", class_=\"date-range\")\n",
    "#studydescription = subSoup.find(\"p\", class_=\"description\")\n",
    "\n",
    "\n",
    "      \n",
    "#<p class=\"date-range\">December 2018 – Present<span class=\"duration\">(4 months)</span><span class=\"location\">Enschede Area, Netherlands</span></p>\n",
    "\n",
    "\n",
    "#Json Datadump\n",
    "personId = 1\n",
    "data = {\n",
    "    str(personId) : { \n",
    "    \"Name\": name,\n",
    "    \"Location\" : town,\n",
    "    \"Industry\" : industry,\n",
    "    \"Profile Summary\" : summary,\n",
    "    \"Experience\": jobs,\n",
    "    \"Education\" : educations,\n",
    "    \"Languages\" : languages,\n",
    "    \"Skills\" : skills,\n",
    "    \"Groups\" : groups\n",
    "       }}\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
